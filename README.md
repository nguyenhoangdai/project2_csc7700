# Neural Language Modeling with RNN, LSTM, and Transformer

This project implements and compares three sequence modelsâ€”**RNN**, **LSTM**, and **Transformer**â€”for text generation using a prompt-completion dataset. Each model is trained separately using a SentencePiece BPE tokenizer.

---

## ğŸ“ Files

- `rnn.py` â€“ training and evaluation code for the RNN model  
- `lstm.py` â€“ training and evaluation code for the LSTM model  
- `transformer.py` â€“ training and evaluation code for the Transformer model  
- `train.jsonl` â€“ training dataset  
- `test.jsonl` â€“ test dataset  

---

## âœ… Requirements

- Python 3.8+
- PyTorch
- SentencePiece
- nltk
- matplotlib

Install requirements via pip:

```bash
pip3 install torch sentencepiece nltk matplotlib

```

## ğŸš€ How to Run

Each model can be trained and evaluated independently:

### Train and evaluate the RNN model
python3 rnn.py

### Train and evaluate the LSTM model
python3 lstm.py

### Train and evaluate the Transformer model
python3 transformer.py

## Each script:

- Loads train.jsonl and test.jsonl
- Trains the model
- Saves loss curve plots
- Prints perplexity and BLEU score
- Generates sample outputs